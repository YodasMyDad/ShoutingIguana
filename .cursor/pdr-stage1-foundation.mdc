# PDR Stage 1 â€” Core Foundation & Basic Crawling

**Working name:** Shouting Iguana  
**Platform:** Windows desktop (WPF, .NET 9)  
**Goal:** Establish core infrastructure and demonstrate basic HTTP crawling with database persistence and CSV export.

---

## Stage 1 Overview

This stage focuses on **proving the fundamentals work**: project management, basic HTTP crawling, SQLite persistence, and simple data export. No JavaScript rendering yet, no plugins yetâ€”just a solid foundation.

**Key Outcome:** User can create a project, crawl a simple site (HTML-only), see results in a grid, and export to CSV.

---

## 1) In Scope (Stage 1)

### Core Infrastructure
- **WPF Application Shell**
  - MaterialDesign theme integration (BlueGrey palette)
  - Basic menu bar: File, View, Crawl, Tools, Help (minimal commands)
  - Toolbar with New Project, Open Project, Start/Stop Crawl buttons
  - Status bar showing project name and basic stats
  
- **Dependency Injection Setup**
  - `Microsoft.Extensions.DependencyInjection`
  - Service registration for repositories, crawl engine, settings
  
- **Logging**
  - Serilog to file (`%LOCALAPPDATA%/ShoutingIguana/logs/`)
  
### Project Management
- **Project Home View**
  - Create new project (name, base URL)
  - Open existing project (from `.db` file)
  - Recent projects list (last 5)
  - Close project
  - Project settings:
    - Base URL (required)
    - Max crawl depth (default: 5)
    - Max URLs to crawl (default: 1000)
    - Respect robots.txt (toggle, default: true)
    - User-agent string
    - Crawl delay (seconds between requests per host)
  
- **File Storage**
  - Projects saved as SQLite files in `%APPDATA%/ShoutingIguana/projects/`
  - File picker dialogs (Ookii.Dialogs)
  
### Database Schema (SQLite + EF Core)
- **Projects** table
  ```
  id (int, PK)
  name (string)
  base_url (string)
  created_utc (datetime)
  last_opened_utc (datetime)
  settings_json (string)  // serialized ProjectSettings
  ```

- **Urls** table
  ```
  id (int, PK)
  project_id (int, FK)
  url (string, indexed)
  normalized_url (string, indexed)
  scheme (string)
  host (string, indexed)
  path (string)
  depth (int)
  discovered_from_url_id (int?, nullable FK)
  first_seen_utc (datetime)
  last_crawled_utc (datetime?)
  status (enum: Pending, Crawling, Completed, Failed)
  http_status (int?)
  content_type (string?)
  content_length (int?)
  robots_allowed (bool?)
  ```

- **Links** table
  ```
  id (int, PK)
  project_id (int, FK)
  from_url_id (int, FK)
  to_url_id (int, FK)
  anchor_text (string?)
  link_type (enum: Hyperlink, Image, Script, Stylesheet, etc.)
  ```

- **CrawlQueue** table
  ```
  id (int, PK)
  project_id (int, FK)
  url (string)
  priority (int)
  depth (int)
  host_key (string, indexed)
  enqueued_utc (datetime)
  state (enum: Queued, InProgress, Completed, Failed)
  ```

- **Headers** table
  ```
  id (int, PK)
  url_id (int, FK)
  name (string)
  value (string)
  ```

### Basic HTTP Crawl Engine
- **Fetcher**
  - Use `HttpClient` with proper user-agent
  - Follow redirects (record chain)
  - Capture response headers, status code, content-type, content-length
  - Download HTML content (text/html only in Stage 1)
  - Timeout: 30 seconds per request
  
- **Robots.txt Support**
  - Fetch and parse robots.txt for each host
  - Respect Disallow rules if setting enabled
  - Respect crawl-delay directive
  - Cache robots.txt per host for session
  
- **Link Extraction**
  - Parse HTML with **HtmlAgilityPack** (fast, easy to use)
  - Extract `<a href>`, `<img src>`, `<link href>`, `<script src>`
  - Normalize URLs (resolve relative, remove fragments, lowercase)
  - Deduplicate discovered URLs
  
- **Queue Management**
  - FIFO queue per host with priority
  - Global concurrency limit (default: 4 simultaneous requests)
  - Per-host politeness delay (respect robots.txt crawl-delay)
  - Seed URLs added with priority 1000, discovered URLs with priority 100
  
- **Persistence**
  - Save every fetched URL to database with status/headers
  - Save discovered links
  - Update queue state
  - Transaction batching for performance (every 10 URLs)
  
- **Error Handling**
  - Polly retry policy: 3 retries with exponential backoff (1s, 2s, 4s)
  - Log errors to Serilog
  - Failed URLs marked with status=Failed, http_status captured
  
### Crawl Dashboard View
- **Progress Display**
  - URLs crawled / total discovered
  - Queue size
  - Active workers (concurrent requests)
  - Errors count
  - Crawl duration (elapsed time)
  
- **Controls**
  - Start Crawl button (disabled during crawl)
  - Stop Crawl button (graceful shutdown)
  - Clear Queue button (when idle)
  
- **Real-time Updates**
  - Update stats every 500ms during crawl
  - Show last 10 crawled URLs in a scrolling list
  
### Findings View (Basic Inventory)
- **URL Inventory Grid**
  - Columns: URL, Status Code, Content Type, Content Length, Depth, Crawled At
  - Sortable by any column
  - Searchable/filterable (text search in URL)
  - Row count display
  - Select rows (multi-select)
  
### Export to CSV
- **CSV Export** (using `CsvHelper`)
  - Export all URLs with selected columns
  - File save dialog
  - Export inventory: url, status_code, content_type, content_length, depth, crawled_at
  - Success notification after export
  
### Basic Menu Structure
**File Menu**
- New Project... (Ctrl+N)
- Open Project... (Ctrl+O)
- Recent Projects â†’ (submenu)
- Close Project (Ctrl+W)
- â”€â”€â”€â”€â”€â”€
- Export to CSV...
- â”€â”€â”€â”€â”€â”€
- Exit

**View Menu**
- Project Home (Ctrl+1)
- Crawl Dashboard (Ctrl+2)
- Findings (Ctrl+3)
- â”€â”€â”€â”€â”€â”€
- Refresh (F5)

**Crawl Menu**
- Start Crawl (F6)
- Stop Crawl (F8)
- â”€â”€â”€â”€â”€â”€
- Clear Queue

**Help Menu**
- View Logs...
- About Shouting Iguana...

---

## 2) Out of Scope (Stage 1)

- Playwright / JavaScript rendering
- Plugin system (SDK, loading, NuGet integration)
- Excel export
- Advanced tasks (broken links, redirects, meta analysis, etc.)
- Pause/Resume crawl
- Resume after crash
- Proxy configuration
- List-mode crawling
- Custom extraction
- Link graph visualization
- Extensions manager UI
- Complex settings UI
- Toolbar (optional, can defer)

---

## 3) Architecture Decisions

### Project Structure
```
ShoutingIguana/
  src/
    ShoutingIguana.Core/          # Core models, interfaces, crawl engine
      Models/
        Project.cs
        Url.cs
        Link.cs
        CrawlQueueItem.cs
      Services/
        ICrawlEngine.cs
        CrawlEngine.cs
        ILinkExtractor.cs
        LinkExtractor.cs
        IRobotsService.cs
        RobotsService.cs
      Configuration/
        ProjectSettings.cs
        CrawlSettings.cs
        
    ShoutingIguana.Data/          # EF Core, SQLite, repositories
      ShoutingIguanaDbContext.cs
      SqliteDbContext.cs
      IShoutingIguanaDbContext.cs
      Migrations/
      Repositories/
        IProjectRepository.cs
        ProjectRepository.cs
        IUrlRepository.cs
        UrlRepository.cs
        
    ShoutingIguana/               # WPF application
      App.xaml
      App.xaml.cs
      Resources/
        Styles.xaml
        Colors.xaml
      Views/
        ProjectHomeView.xaml
        CrawlDashboardView.xaml
        FindingsView.xaml
      ViewModels/
        MainViewModel.cs
        ProjectHomeViewModel.cs
        CrawlDashboardViewModel.cs
        FindingsViewModel.cs
      Services/
        INavigationService.cs
        NavigationService.cs
        IExportService.cs
        CsvExportService.cs
      Converters/
      MainWindow.xaml
```

### Technology Stack (Stage 1)
```xml
<PackageReference Include="MaterialDesignThemes" Version="5.3.0" />
<PackageReference Include="MaterialDesignColors" Version="5.3.0" />
<PackageReference Include="CommunityToolkit.Mvvm" Version="8.4.0" />
<PackageReference Include="Microsoft.Extensions.DependencyInjection" Version="9.0.9" />
<PackageReference Include="Microsoft.Extensions.Hosting" Version="9.0.9" />
<PackageReference Include="Serilog" Version="4.3.0" />
<PackageReference Include="Serilog.Sinks.File" Version="7.0.0" />
<PackageReference Include="Microsoft.EntityFrameworkCore.Sqlite" Version="9.0.9" />
<PackageReference Include="Microsoft.EntityFrameworkCore.Design" Version="9.0.9" />
<PackageReference Include="HtmlAgilityPack" Version="1.12.4" />
<PackageReference Include="Polly" Version="8.6.4" />
<PackageReference Include="CsvHelper" Version="33.1.0" />
<PackageReference Include="Ookii.Dialogs.Wpf" Version="5.0.1" />
```

---

## 4) Testing Criteria (Stage 1 Complete)

âœ… **Application starts without errors**
- Main window loads with MaterialDesign theme applied
- Menu bar and basic UI elements render correctly
- Log file created in expected location

âœ… **Project Management**
- Can create new project with name and base URL
- Project file (.db) saved to AppData folder
- Can open existing project from file picker
- Recent projects list populates and works
- Can close project and return to welcome screen

âœ… **Database**
- Tables created on first run (migrations applied)
- Can insert and query Projects, Urls, Links, CrawlQueue, Headers
- No EF Core errors or warnings

âœ… **Basic Crawl**
- Enter `http://example.com` as base URL
- Click Start Crawl
- Crawl completes successfully
- At least 10 URLs discovered and fetched
- Status codes captured correctly
- Content types captured correctly

âœ… **Robots.txt Respect**
- Fetch robots.txt for test domain
- Verify Disallow rules are respected when setting enabled
- Verify crawl-delay is respected
- Verify toggle works (can ignore robots.txt when disabled)

âœ… **Link Extraction**
- Crawl a page with multiple links
- All `<a href>` links extracted and normalized
- Links saved to Links table with correct from/to relationships
- No duplicate URLs queued

âœ… **Crawl Dashboard**
- Real-time stats update during crawl
- Shows URLs crawled, queue size, errors
- Stop button gracefully halts crawl mid-stream
- Can restart crawl after stop

âœ… **Findings View**
- URL inventory grid populates with crawled URLs
- All columns display correct data
- Sort by column works
- Search/filter works
- Can select multiple rows

âœ… **CSV Export**
- Click Export to CSV
- File save dialog appears
- CSV file created with all URLs
- Open CSV in Excelâ€”data is correct and complete
- Confirmation message shown after export

âœ… **Error Handling**
- Crawl a non-existent URL (404)â€”error captured, app doesn't crash
- Crawl unreachable domainâ€”timeout handled gracefully
- Invalid base URLâ€”validation error shown

---

## 5) UI/UX Notes for Stage 1

### Welcome Screen (No Project Open)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Shouting Iguana                      â”‚
â”‚                                                      â”‚
â”‚         [Icon/Logo]                                  â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  ğŸ†• New Project                         â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚  ğŸ“‚ Open Project                        â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                      â”‚
â”‚   Recent Projects:                                  â”‚
â”‚   â€¢ MyWebsite (opened 2 hours ago)                  â”‚
â”‚   â€¢ ClientSite (opened yesterday)                   â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Home View (After Creating Project)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Project: MyWebsite                                  â”‚
â”‚                                                      â”‚
â”‚  Base URL: [http://example.com          ] Required  â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€ Crawl Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Max Depth: [5     ]                      â”‚       â”‚
â”‚  â”‚ Max URLs:  [1000  ]                      â”‚       â”‚
â”‚  â”‚ Delay (s): [1.0   ]                      â”‚       â”‚
â”‚  â”‚ â˜‘ Respect robots.txt                    â”‚       â”‚
â”‚  â”‚                                          â”‚       â”‚
â”‚  â”‚ User Agent:                              â”‚       â”‚
â”‚  â”‚ [ShoutingIguana/1.0                   ] â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                      â”‚
â”‚  [ğŸ’¾ Save Settings]  [â–¶ï¸ Start Crawl]              â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Crawl Dashboard (During Crawl)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Crawl Progress                                      â”‚
â”‚                                                      â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  65%                    â”‚
â”‚                                                      â”‚
â”‚  ğŸ“Š URLs Crawled: 156 / 240 discovered              â”‚
â”‚  ğŸ“‹ Queue Size: 84                                   â”‚
â”‚  âš¡ Active Workers: 4                               â”‚
â”‚  âŒ Errors: 3                                       â”‚
â”‚  â±ï¸ Elapsed: 00:02:34                              â”‚
â”‚                                                      â”‚
â”‚  Recently Crawled:                                   â”‚
â”‚  âœ“ http://example.com/page1 (200 OK)               â”‚
â”‚  âœ“ http://example.com/page2 (200 OK)               â”‚
â”‚  âœ— http://example.com/missing (404 Not Found)      â”‚
â”‚  ...                                                 â”‚
â”‚                                                      â”‚
â”‚  [â¹ï¸ Stop Crawl]                                   â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Findings View (After Crawl)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  URL Inventory                     [ğŸ” Search] [â¬‡ï¸CSV]â”‚
â”‚                                                      â”‚
â”‚  Showing 156 URLs                                    â”‚
â”‚                                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ URL â†•         â”‚Statusâ”‚Type     â”‚Size  â”‚Depth  â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ /page1        â”‚ 200  â”‚text/htmlâ”‚ 4523 â”‚ 1     â”‚ â”‚
â”‚ â”‚ /page2        â”‚ 200  â”‚text/htmlâ”‚ 3201 â”‚ 1     â”‚ â”‚
â”‚ â”‚ /about        â”‚ 200  â”‚text/htmlâ”‚ 2890 â”‚ 2     â”‚ â”‚
â”‚ â”‚ /missing      â”‚ 404  â”‚text/htmlâ”‚ 512  â”‚ 2     â”‚ â”‚
â”‚ â”‚ ...                                             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6) Implementation Order (Suggested)

1. **Setup WPF shell** with MaterialDesign theme, basic window, menu structure
2. **Setup DI container** in App.xaml.cs, register services
3. **Setup Serilog** logging to file
4. **Create EF Core models and DbContext** (Projects, Urls, Links, Queue, Headers tables)
5. **Add initial migration** and verify database creation
6. **Build Project Home View + ViewModel** with create/open/save project logic
7. **Implement project file storage** (save/load SQLite files)
8. **Build basic HTTP fetcher** with HttpClient
9. **Implement robots.txt fetcher and parser**
10. **Implement AngleSharp link extractor**
11. **Build crawl engine** (queue management, fetcher integration, link extraction, politeness)
12. **Build Crawl Dashboard View** with progress updates
13. **Test basic crawl end-to-end** on example.com
14. **Build Findings View** with URL inventory grid
15. **Implement CSV export service**
16. **Polish UI**, add keyboard shortcuts, test all menu commands
17. **Run full acceptance tests**

---

## 7) Success Criteria Summary

**Stage 1 is complete when:**

1. âœ… Application launches with modern MaterialDesign UI
2. âœ… User can create and manage projects (create, open, save, close)
3. âœ… User can configure basic crawl settings (URL, depth, robots.txt, delays)
4. âœ… Crawl engine successfully crawls a simple HTML site (no JS needed)
5. âœ… Robots.txt is fetched and respected (or ignored based on setting)
6. âœ… Links are extracted and followed up to configured depth
7. âœ… All URLs, links, headers saved to SQLite database
8. âœ… Crawl Dashboard shows real-time progress with accurate stats
9. âœ… Findings View displays crawled URL inventory in sortable grid
10. âœ… User can export URL inventory to CSV
11. âœ… Application handles errors gracefully (404s, timeouts, bad URLs)
12. âœ… Logs written to file for debugging

**When these work, Stage 1 is DONE and you can move to Stage 2.**

---

## 8) Known Limitations (Acceptable in Stage 1)

- No JavaScript rendering (Playwright comes in Stage 2)
- No pause/resume crawl (just start/stop)
- No crash recovery
- No Excel export (CSV only)
- No advanced analysis (broken links, meta analysis, etc.)
- No plugins or extensibility
- No proxy support
- No list-mode crawling
- Simple UI (no toolbar, minimal menus)
- No auto-save during crawl (data persisted per URL, but not checkpointed for resume)

**These are explicitly deferred to Stage 2 and Stage 3.**
